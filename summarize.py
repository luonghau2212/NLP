# -*- coding: utf-8 -*-
"""TT_URL_VN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1daHS-53_unYdF6ee3pIzk1jcfA4gEnh_
"""

#pip install beautifulsoup4
#pip install lxml

#lấy dữ liệu từ web
import bs4 as bs
import urllib.request
import re

scraped_data = urllib.request.urlopen('https://danviet.vn/nho-mai-ngay-dau-tac-nghiep-ve-tran-lu-quet-lich-su-o-tay-bac-20240314225610812.htm') #sử dụng urlopen - 1 chức năng của urllib.request để cào dữ liệu
article = scraped_data.read() # sử dụng hàm read() để đọc dữ liệu
parsed_article = bs.BeautifulSoup(article,'lxml')
paragraphs = parsed_article.find_all('p') #Để truy xuất văn bản, 'p'vì tất cả nội dung của bài viết đều được đặt bên trong các <p>
# hàm find_all sẽ trả về tất cả các đoạn văn trong bài viết dưới dạng danh sách.
article_text = ""
for p in paragraphs:
    article_text += p.text # kết hợp tất cả các đoạn lại thành 1 bài viết.

#xóa dấu ngoặc vuông và dấu cách dư thừa
article_text = re.sub(r'\[[0-9]*\]', ' ', article_text)
# r ở đầu chuỗi là để biểu thị chuỗi raw
# \[ và \] là các ký tự trùng với dấu ngoặc vuông [ và ].
# [0-9]* là một mẫu để tìm kiếm bất kỳ chuỗi con nào chứa các chữ số từ 0 đến 9
# * biểu thị cho việc có thể xuất hiện 0 hoặc nhiều lần của mẫu trước đó.
article_text = re.sub(r'\s+', ' ', article_text)

# Xóa các chữ số và kí tự đặc biệt
formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )
# re.sub: thay thế tất cả các xuất hiện của mẫu được chỉ định trong một chuỗi bằng một chuỗi khác.
#[^a-zA-Z]: nếu các kí tự không phải là từ a đến z hoặc từ A đến Z thì thay thế bằng dấu cách
formatted_article_text = re.sub(r'\s+', ' ', formatted_article_text)
# thay thế các dấu cách liên tiếp bằng 1 dấu cách

import nltk
nltk.download('punkt') # punkt tokenizer models: chia văn bản thành cách câu.

#chuyển đổi văn bản thành câu
sentence_list = nltk.sent_tokenize(article_text)

pip install stopwordsiso #thư viện stopword vietnamese

#loại bỏ stopwords

from stopwordsiso import stopwords

def remove_vietnamese_stopwords(text):
    stopwords_list = stopwords("vi")
    words = text.split()
    cleaned_words = [word for word in words if word.lower() not in stopwords_list]
    cleaned_text = ' '.join(cleaned_words)
    return cleaned_text

cleaned_text = remove_vietnamese_stopwords(formatted_article_text)
#print(cleaned_text)

"""formatted_article_text được thay thế bằng clean_text (Đã xóa bỏ stopwords) """

# tìm tần suất xuất hiện của từng từ

word_frequencies = {} # tạo một từ điển rỗng để lưu tần suất của các từ
for word in nltk.word_tokenize(formatted_article_text): # duyệt qua từng từ trong formatted_article_text - có phương thức tách từ: word_tokenize()
    if word in cleaned_text:
        if word not in word_frequencies.keys(): #Kiểm tra xem từ đã xuất hiện trong từ điển word_frequencies chưa
            word_frequencies[word] = 1 # nếu chưa có thì Thêm từ word vào từ điển word_frequencies
        else:
            word_frequencies[word] += 1 #tăng tần suất lên 1.

#chuẩn hóa tần suất xuất hiện của các từ trong văn bản. Bằng cách chia tần suất xuất hiện của mỗi từ cho tần suất xuất hiện của từ có tần suất xuất hiện cao nhất trong văn bản
maximum_frequncy = max(word_frequencies.values())

for word in word_frequencies.keys():
    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)

"""#hiển thị bảng chuẩn hóa trên bằng thư viện pandas - nếu cần
import pandas as pd
# Chuyển đổi từ điển thành DataFrame của Pandas
df_word_frequencies = pd.DataFrame(list(word_frequencies.items()), columns=['Word', 'Frequency'])
print(df_word_frequencies)"""

#tính điểm cho câu.
sentence_scores = {} #Tạo một từ điển trống để lưu trữ điểm của mỗi câu.
for sent in sentence_list:#sentence_list: được giả định là một câu trong văn bản.
    for word in nltk.word_tokenize(sent.lower()): #chuyển về dạng chữ thường để đảm bảo tính nhất quán.
        if word in word_frequencies.keys():
            if len(sent.split(' ')) < 30:
                if sent not in sentence_scores.keys():
                    sentence_scores[sent] = word_frequencies[word] # nếu chưa có trong sentence_scores thì sẽ thêm bằng điểm bằng tần suất xuất hiện của từ hiện tại
                else:
                    sentence_scores[sent] += word_frequencies[word]

import heapq #để thao tác với cấu trúc dữ liệu heap (ngăn xếp)
summary_sentences = heapq.nlargest(20, sentence_scores, key=sentence_scores.get) #nlargest(): để lấy ra các phần tử lớn nhất từ một iterable.
#'7': lấy 7 câu có số điểm cao nhất
summary = ' '.join(summary_sentences) #kết nối các câu thành văn bản tóm tắt

print(summary)

"""Comment: LuongVanhau"""

